import requests
from bs4 import BeautifulSoup
import pandas as pd

# Create a variable to store articles
search_results = []

# Define the root URL
root = "https://www.workcover.wa.gov.au/news/"
response = requests.get(root)
soup = BeautifulSoup(response.content, "html.parser")

# Locate the box that contains all articles
box = soup.find('div', class_='container site-inner')

# Extract the links that correspond to the articles found
links = set()  # Using a set to store unique links
for link in box.find_all('a', href=True):
    href = link['href']
    if '/news/' in href:  # Ensure the link contains '/news/' as intended
        links.add(href)

# Remove the root URL from the links list
links = [link for link in links if link != root]

# Loop through each link and extract needed data for headline, description, publish date
for link in links:
    # Fetch HTML content of each link
    article_response = requests.get(link)
    article_soup = BeautifulSoup(article_response.content, "html.parser")

    # Extract headline, content, date from the article page
    headline_element = article_soup.find('h1', itemprop='headline', class_='entry-title')
    headline = headline_element.text.strip() if headline_element else "N/A"
    content = article_soup.find('div', 'entry-content').text.strip()
    date = article_soup.find('p', 'article-date').text.strip()
    article_url = f'{root}{link}'

    # Append extracted data to search_results list
    search_results.append([headline, content, date, article_url])

# Convert search_results list to DataFrame
wcwa_df = pd.DataFrame(search_results, columns=['Headline', 'Content', 'Date', 'Reference URL'])

# Display DataFrame
wcwa_df
